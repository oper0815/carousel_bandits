{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import ContextualEnvironment\n",
    "from policies import KLUCBSegmentPolicy, RandomPolicy, ExploreThenCommitSegmentPolicy, EpsilonGreedySegmentPolicy, TSSegmentPolicy, LinearTSPolicy\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_policies(policies_name, user_segment, user_features, n_playlists):\n",
    "    # Please see section 3.3 of RecSys paper for a description of policies\n",
    "    POLICIES_SETTINGS = {\n",
    "        'random' : RandomPolicy(n_playlists),\n",
    "        'etc-seg-explore' : ExploreThenCommitSegmentPolicy(user_segment, n_playlists, min_n = 100, cascade_model = True),\n",
    "        'etc-seg-exploit' : ExploreThenCommitSegmentPolicy(user_segment, n_playlists, min_n = 20, cascade_model = True),\n",
    "        'epsilon-greedy-explore' : EpsilonGreedySegmentPolicy(user_segment, n_playlists, epsilon = 0.1, cascade_model = True),\n",
    "        'epsilon-greedy-exploit' : EpsilonGreedySegmentPolicy(user_segment, n_playlists, epsilon = 0.01, cascade_model = True),\n",
    "        'kl-ucb-seg' : KLUCBSegmentPolicy(user_segment, n_playlists, cascade_model = True),\n",
    "        'ts-seg-naive' : TSSegmentPolicy(user_segment, n_playlists, alpha_zero = 1, beta_zero = 1, cascade_model = True),\n",
    "        'ts-seg-pessimistic' : TSSegmentPolicy(user_segment, n_playlists, alpha_zero = 1, beta_zero = 99, cascade_model = True),\n",
    "        'ts-lin-naive' : LinearTSPolicy(user_features, n_playlists, bias = 0.0, cascade_model = True),\n",
    "        'ts-lin-pessimistic' : LinearTSPolicy(user_features, n_playlists, bias = -5.0, cascade_model = True),\n",
    "        # Versions of epsilon-greedy-explore and ts-seg-pessimistic WITHOUT cascade model\n",
    "        'epsilon-greedy-explore-no-cascade' : EpsilonGreedySegmentPolicy(user_segment, n_playlists, epsilon = 0.1, cascade_model = False),\n",
    "        'ts-seg-pessimistic-no-cascade' : TSSegmentPolicy(user_segment, n_playlists, alpha_zero = 1, beta_zero = 99, cascade_model = False)\n",
    "    }\n",
    "\n",
    "    return [POLICIES_SETTINGS[name] for name in policies_name]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--users_path\", type = str, default = \"data/user_features.csv\", required = False,\n",
    "                    help = \"Path to user features file\")\n",
    "parser.add_argument(\"--playlists_path\", type = str, default = \"data/playlist_features.csv\", required = False,\n",
    "                    help = \"Path to playlist features file\")\n",
    "parser.add_argument(\"--output_path\", type = str, default = \"results.json\", required = False,\n",
    "                    help = \"Path to json file to save regret values\")\n",
    "parser.add_argument(\"--policies\", type = str, default = \"ts-lin-pessimistic\", required = False,\n",
    "                    help = \"Bandit algorithms to evaluate, separated by commas\")\n",
    "parser.add_argument(\"--n_recos\", type = int, default = 12, required = False,\n",
    "                    help = \"Number of slots L in the carousel i.e. number of recommendations to provide\")\n",
    "parser.add_argument(\"--l_init\", type = int, default = 3, required = False,\n",
    "                    help = \"Number of slots L_init initially visible in the carousel\")\n",
    "parser.add_argument(\"--n_users_per_round\", type = int, default = 20, required = False,\n",
    "                    help = \"Number of users randomly selected (with replacement) per round\")\n",
    "parser.add_argument(\"--n_rounds\", type = int, default = 100, required = False,\n",
    "                    help = \"Number of simulated rounds\")\n",
    "parser.add_argument(\"--print_every\", type = int, default = 10, required = False,\n",
    "                    help = \"Print cumulative regrets every 'print_every' round\")\n",
    "\n",
    "args = parser.parse_args(args = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists_df = pd.read_csv('data/playlist_features.csv')\n",
    "users_df = pd.read_csv('data/user_features_small.csv')\n",
    "n_users = len(users_df)\n",
    "n_playlists = len(playlists_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_recos = args.n_recos\n",
    "print_every = args.print_every\n",
    "\n",
    "user_features = np.array(users_df.drop([\"segment\"], axis = 1)) # segment 제외\n",
    "user_features = np.concatenate([user_features, np.ones((n_users,1))], axis = 1) # feature 맨 뒤에 값 추가\n",
    "playlist_features = np.array(playlists_df)\n",
    "\n",
    "user_segment = np.array(users_df.segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_env = ContextualEnvironment(user_features, playlist_features, user_segment, n_recos)\n",
    "\n",
    "policies_name = args.policies.split(\",\")\n",
    "policies = set_policies(policies_name, user_segment, user_features, n_playlists)                    # init 수행\n",
    "n_policies = len(policies)\n",
    "n_users_per_round = args.n_users_per_round\n",
    "n_rounds = args.n_rounds\n",
    "overall_rewards = np.zeros((n_policies, n_rounds))\n",
    "overall_optimal_reward = np.zeros(n_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:STARTING SIMULATIONS\n",
      "INFO:__main__:for 100 rounds, with 20 users per round (randomly drawn with replacement)\n",
      " \n",
      "\n",
      "INFO:__main__:Round: 1/100. Elapsed time: 0.164642 sec.\n",
      "INFO:__main__:Cumulative regrets: \n",
      "\tts-lin-pessimistic : 10.635216074036354 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"STARTING SIMULATIONS\")\n",
    "logger.info(\"for %d rounds, with %d users per round (randomly drawn with replacement)\\n \\n\" % (n_rounds, n_users_per_round))\n",
    "start_time = time.time()\n",
    "\n",
    "i = 0\n",
    "# Select batch of n_users_per_round users\n",
    "user_ids = np.random.choice(range(n_users), n_users_per_round)                                  # 전체 유저에서 n_users_per_round 크기 만큼 샘플링 / 중복 유저도 가능한데...?\n",
    "overall_optimal_reward[i] = np.take(cont_env.th_rewards, user_ids).sum()                        # overall_optimal_reward[i] = batch user에 있는 사람들의 reward 합\n",
    "# Iterate over all policies\n",
    "for j in range(n_policies):\n",
    "    # Compute n_recos recommendations\n",
    "    recos = policies[j].recommend_to_users_batch(user_ids, args.n_recos, args.l_init)           # user_ids(배치 크기)에 있는 유저에 대한 추천리스트 / (20000 x 12)\n",
    "    # Compute rewards\n",
    "    rewards = cont_env.simulate_batch_users_reward(batch_user_ids= user_ids, batch_recos=recos) # Sample 유저에 대한 reward 반환\n",
    "    # Update policy based on rewards\n",
    "    policies[j].update_policy(user_ids, recos, rewards, args.l_init)                            \n",
    "    overall_rewards[j,i] = rewards.sum()\n",
    "# Print info\n",
    "if i == 0 or (i+1) % print_every == 0 or i+1 == n_rounds:\n",
    "    logger.info(\"Round: %d/%d. Elapsed time: %f sec.\" % (i+1, n_rounds, time.time() - start_time))\n",
    "    logger.info(\"Cumulative regrets: \\n%s \\n\" % \"\\n\".join([\"\t%s : %s\" % (policies_name[j], str(np.sum(overall_optimal_reward - overall_rewards[j]))) for j in range(n_policies)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:STARTING SIMULATIONS\n",
      "INFO:__main__:for 100 rounds, with 20 users per round (randomly drawn with replacement)\n",
      " \n",
      "\n",
      "INFO:__main__:Round: 1/100. Elapsed time: 0.169040 sec.\n",
      "INFO:__main__:Cumulative regrets: \n",
      "\tts-lin-pessimistic : 10.352586198915116 \n",
      "\n",
      "INFO:__main__:Round: 10/100. Elapsed time: 1.408333 sec.\n",
      "INFO:__main__:Cumulative regrets: \n",
      "\tts-lin-pessimistic : 82.71741827346239 \n",
      "\n",
      "INFO:__main__:Round: 20/100. Elapsed time: 2.176514 sec.\n",
      "INFO:__main__:Cumulative regrets: \n",
      "\tts-lin-pessimistic : 174.9784613134419 \n",
      "\n",
      "INFO:__main__:Round: 30/100. Elapsed time: 2.813665 sec.\n",
      "INFO:__main__:Cumulative regrets: \n",
      "\tts-lin-pessimistic : 260.5503801048198 \n",
      "\n",
      "INFO:__main__:Round: 40/100. Elapsed time: 3.455817 sec.\n",
      "INFO:__main__:Cumulative regrets: \n",
      "\tts-lin-pessimistic : 342.276439639027 \n",
      "\n",
      "INFO:__main__:Round: 50/100. Elapsed time: 4.086966 sec.\n",
      "INFO:__main__:Cumulative regrets: \n",
      "\tts-lin-pessimistic : 429.88134390256096 \n",
      "\n",
      "INFO:__main__:Round: 60/100. Elapsed time: 4.716115 sec.\n",
      "INFO:__main__:Cumulative regrets: \n",
      "\tts-lin-pessimistic : 524.3025531587036 \n",
      "\n",
      "INFO:__main__:Round: 70/100. Elapsed time: 5.328259 sec.\n",
      "INFO:__main__:Cumulative regrets: \n",
      "\tts-lin-pessimistic : 613.1720876464994 \n",
      "\n",
      "INFO:__main__:Round: 80/100. Elapsed time: 5.984415 sec.\n",
      "INFO:__main__:Cumulative regrets: \n",
      "\tts-lin-pessimistic : 689.796323724642 \n",
      "\n",
      "INFO:__main__:Round: 90/100. Elapsed time: 6.609563 sec.\n",
      "INFO:__main__:Cumulative regrets: \n",
      "\tts-lin-pessimistic : 777.1938857855108 \n",
      "\n",
      "INFO:__main__:Round: 100/100. Elapsed time: 7.234125 sec.\n",
      "INFO:__main__:Cumulative regrets: \n",
      "\tts-lin-pessimistic : 855.9763101467715 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"STARTING SIMULATIONS\")\n",
    "logger.info(\"for %d rounds, with %d users per round (randomly drawn with replacement)\\n \\n\" % (n_rounds, n_users_per_round))\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(n_rounds):\n",
    "    # Select batch of n_users_per_round users\n",
    "    user_ids = np.random.choice(range(n_users), n_users_per_round)                                  # 전체 유저에서 n_users_per_round 크기 만큼 샘플링 / 중복 유저도 가능한데...?\n",
    "    overall_optimal_reward[i] = np.take(cont_env.th_rewards, user_ids).sum()                        # overall_optimal_reward[i] = batch user에 있는 사람들의 reward 합\n",
    "    # Iterate over all policies\n",
    "    for j in range(n_policies):\n",
    "        # Compute n_recos recommendations\n",
    "        recos = policies[j].recommend_to_users_batch(user_ids, args.n_recos, args.l_init)           # user_ids(배치 크기)에 있는 유저에 대한 추천리스트 / (20000 x 12)\n",
    "        # Compute rewards\n",
    "        rewards = cont_env.simulate_batch_users_reward(batch_user_ids= user_ids, batch_recos=recos) # Sample 유저에 대한 reward 반환\n",
    "        # Update policy based on rewards\n",
    "        policies[j].update_policy(user_ids, recos, rewards, args.l_init)                            \n",
    "        overall_rewards[j,i] = rewards.sum()\n",
    "    # Print info\n",
    "    if i == 0 or (i+1) % print_every == 0 or i+1 == n_rounds:\n",
    "        logger.info(\"Round: %d/%d. Elapsed time: %f sec.\" % (i+1, n_rounds, time.time() - start_time))\n",
    "        logger.info(\"Cumulative regrets: \\n%s \\n\" % \"\\n\".join([\"\t%s : %s\" % (policies_name[j], str(np.sum(overall_optimal_reward - overall_rewards[j]))) for j in range(n_policies)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
